[{"content":"Introduction Tokenization is a process that replaces sensitive data with non-sensitive data, typically using a reference token that maps back to the original data. This is a practice that is part of secrets management, a method of secretly storing, accessing, and managing digital secrets.\nIn my pursuit of learning more about DevSecOps, I wanted to get my hands on some Kubernetes work to learn about the deployment, scaling, and management of containerized applications. Kubernetes was chosen for its ability to manage containerized applications specifically at scale. This makes it a valuable skill to have in DevOps.\nIn the lab covered in this post, we will create a simplified version of a vaulted tokenization service, containerize it, and host it using Kubernetes.\nTechnology Stack For the tokenization service, I created a simple flask API that tokenizes information, stores it, and allows the information to be detokenized and retrieved. This application came to mind as a simple API I could set up, and is even reminiscent of the capstone work from senior year!\nTo set up Kubernetes I used Minikube. Minikube is an open source tool that allows developers to spin up a local cluster, eliminating the cost and complexity of the cloud when testing. For development, this tool is invaluable.\nFinally for containerization, like last weeks Lab I am using Docker.\nTokenization Workflow The tokenization service supports two functions:\nTokenization: Users send a POST request with a name and data. The data is encrypted using a public key and stored in a YAML file alongside the token name. Detokenization: Users send a POST request with a name. The token associated with the name is retrieved from the YAML file, decrypted using the private key, and returned to the user. Setting Up Kubernetes To host this with Kubernetes, we first create a cluster using Minikube.\nminikube start –driver=docker This will spin up the Kubernetes cluster, preparing us to load pods with containers to run and administrate. To achieve this, we must create YAML files to configure our pods, services, and deployment.\nThe first YAML file is the deployment.yaml file. Some of the most important specifications in this file are the declaration of container name and image for our application, container port for its networking, and volume mounts for access to saving tokens. More specifics on this can be found here\ndeployment.yaml:\napiVersion: apps/v1 kind: Deployment metadata: name: tokenization-service spec: replicas: 1 selector: matchLabels: app: tokenization-service template: metadata: labels: app: tokenization-service spec: containers: - name: tokenization-service image: tokenization-service:latest imagePullPolicy: Never ports: - containerPort: 5000 volumeMounts: - name: app-data mountPath: /app/saves volumes: - name: app-data emptyDir: {} NOTE: the volumeMounts ensures the file for token storage is persistent. This deployment also has imagePullPolicy: Never so as to not hang in attempting to pull an image.\nThe next YAML file is the service.yaml file. This, like the previous file, offers configuration but for the service API for Kubernetes. A service makes an endpoint and declares how the pods are accessible. In our case, we specify some metadata, port protocol, and port numbers.\nservice.yaml:\napiVersion: v1 kind: Service metadata: name: tokenization-service spec: selector: app: tokenization-service ports: - protocol: TCP port: 5000 targetPort: 5000 type: NodePort Deployment For this lab environment, the Docker image is loaded directly into the Minikube cluster. In a production environment, images are typically hosted in a container registry such as Docker Hub or AWS ECR.\nTo begin deployment, the docker image is built and loaded locally.\ndocker build -t tokenization-service . minikube image load tokenization-service:latest We then apply our Kubernetes configurations, deployment.yaml and service.yaml.\nkubectl apply -f deployment.yaml kubectl apply -f service.yaml This will begin the creation of the pods we specified and start containers for the service. Just like that a single node Kubernetes cluster running a custom application.\nTokenization Detokenization Lessons Learned Kubernetes is highly configurable. This means there are many opportunities to make mistakes. I spent a lot of time chasing errors and going over configuration files. Because of this granularity in configuration there exists many Kubernetes tools! This article is a great introduciton to that world.\nConclusion This project provided me with valuable experience in Kubernetes and DevSecOps principles. I have a much better understanding of Kubernetes and some new found comfort working with it.\n","permalink":"http://karltrowbridge1.github.io/blog/buildingavaultedtokenizationservicewithkubernetes/building-a-vaulted-tokenization-service-with-kubernetes/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eTokenization is a process that replaces sensitive data with non-sensitive data, typically using a reference token that maps back to the original data. This is a practice that is part of secrets management, a method of secretly storing, accessing, and managing digital secrets.\u003c/p\u003e\n\u003cp\u003eIn my pursuit of learning more about DevSecOps, I wanted to get my hands on some Kubernetes work to learn about the deployment, scaling, and management of containerized applications. Kubernetes was chosen for its ability to manage containerized applications specifically at scale. This makes it a valuable skill to have in DevOps.\u003c/p\u003e","title":"Building a Vaulted Tokenization Service With Kubernetes"},{"content":"Introduction A few friends and I are working on an open-source project called SpotBot. The best description of the project is \u0026ldquo;an overengineered shared playlist.\u0026rdquo; While working on this project, things became a little convoluted. Initially, we had issues with the application running properly on Linux. I took the initiative to test the entire application on a virtual machine running Debian to ensure everything functioned correctly in a Linux environment. Here\u0026rsquo;s how the process went:\nStart the VM Delete the old repo that was locally cloned from GitHub Enter the VENV Re-clone the repo that needs to be tested Move the configuration file to the directory Begin testing I imgaine anyone who works in automation is disappointed.\nDocker quickly became the obvious choice to automate this entire process. Through this endeavor, I learned the benefits of Docker\u0026rsquo;s lightweight nature, the usability of volumes for a better user experience, and how application concerns often extend beyond the end user.\nA link to the SpotBot Docker testing container can be found here.\nLightweight Containers Initially, the container I created utilized Ubuntu, which was very straightforward to work with. Many of the tools the application relies on were already included in the image, making build times fast.\nAfter sharing my work with a friend more experienced with Docker, he provided feedback that shaped the lessons in this post. He suggested using Alpine or Manjaro Linux instead.\nI transitioned to Alpine, which required modifying several aspects of the Dockerfile. While this added steps to install additional packages during the build process, it resulted in a container that was much more efficient to run and deploy.\nVolumes To run SpotBot, the administrator needs to configure and provide a JSON file containing setup information. In my first attempt at creating the container, I copied this file into the container via the Dockerfile. However, my friend pointed out that this approach was not best practice. Instead, utilizing a volume allows users to build the image once and run the container with the flexibility to update setup files without rebuilding.\nAfter adjusting the configuration, users can now freely add or change a setup file on the fly via a volume.\nFor ease of use, I created a directory, /app/setup/, for the volume to mount. The container\u0026rsquo;s run command now requires a volume declaration: -v /path/to/local/dirContainingSetupFile:/app/setup\nThis allows the startup shell script to reference the setup.json file provided by the user.\nIssues in Automation Process An interestign issue with the flask applicaiton used for authentication with the Spotify servers was made apparant when the prompt I had created to close the application after authentication got burried by logs from the flask application.\nTo terminate the process after successful authentication, I used a read statement in the bash script. In classic primitive programming fashion, the application closes when the user hits .\nWhen running SpotBot on bare metal, the webserver is stopped via a keyboard interrupt. While this approach works well enough, it’s a low-priority item on the to-do list. However, creating this Docker container taught me the importance of thinking beyond just the end user to consider system integration and automation. Where a keyboard interrupt worked manually, automated systems may struggle with such a design. This was a small but valuable takeaway.\nConclusion Creating and using this container for fast testing has been an excellent time-saver and learning experience. The ability to spin up and tear down an entire environment in seconds is fantastic. Passing this container between developers has significantly reduced the \u0026ldquo;it works on my machine\u0026rdquo; scenarios that lead to broken code being pushed.\nFurthermore, this simple container can serve as a platform to jumpstart a production container, simplifying administration for end users.\nEnjoying this project has inspired me to explore creating a CI/CD pipeline for SpotBot. One step at a time.\n","permalink":"http://karltrowbridge1.github.io/blog/spotbotdocker/spotbot_docker/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eA few friends and I are working on an open-source project called \u003ca href=\"https://karltrowbridge1.github.io/projects/spotbot/\"\u003eSpotBot\u003c/a\u003e. The best description of the project is \u0026ldquo;an overengineered shared playlist.\u0026rdquo; While working on this project, things became a little convoluted. Initially, we had issues with the application running properly on Linux. I took the initiative to test the entire application on a virtual machine running Debian to ensure everything functioned correctly in a Linux environment. Here\u0026rsquo;s how the process went:\u003c/p\u003e","title":"SpotBot Docker Container"},{"content":"What is John the Ripper John the Ripper is a password cracker that supports many types of hashes or ciphers. The flexible software is able to run on several different kinds of operating systems utilizing CPU power or GPU power in some cases. A link to the GitHub repo for John the Ripper can be found here.\nWhen testing the strength or security of passwords during a penetration test/security audit a password cracker can identify weak credentials to highlight vulnerabilities within an organization and its infrastructure. Tools like John the Ripper can also prove useful in a forensic investigation where an unknown password may hold investigators from important data.\nIn my case, during a CTF for the PJPT I discovered a zip file that was password protected. A lot less exciting, but still fun and educational!\nUsing John John the Ripper has a unique but simple way of documenting its functionality. After downloading and unzipping the app, there are 3 directories; doc, etc, and run.\ndoc contains the documentation related to John the Ripper and how to use it.\netc holds vendor files - I didn\u0026rsquo;t touch these.\nand run has the executables files, wordlists and other resources.\nFirst, I needed to get the hash from the file. In the documentation there is a file called README-ZIP.txt which contains all we need to know about cracking zip files with this software, and it is dirt simple\nTo extract the hash for John to use, we run a program called zip2john in the run directory. The first argument is the zip file, and we can redirect this output into a new file like so:\nWith this file I can run John and feed it the hash contents for it to crack.\nAnd success! The password is java101. Onto the next step\u0026hellip;\n","permalink":"http://karltrowbridge1.github.io/blog/john/john-the-ripper/","summary":"\u003ch1 id=\"what-is-john-the-ripper\"\u003eWhat is John the Ripper\u003c/h1\u003e\n\u003cp\u003eJohn the Ripper is a password cracker that supports many types of hashes or ciphers. The flexible software is able to run on several different kinds of operating systems utilizing CPU power or GPU power in some cases. A link to the GitHub repo for John the Ripper can be found \u003ca href=\"https://github.com/openwall/john\"\u003ehere\u003c/a\u003e.\u003c/p\u003e\n\u003cp\u003eWhen testing the strength or security of passwords during a penetration test/security audit a password cracker can identify weak credentials to highlight vulnerabilities within an organization and its infrastructure. Tools like John the Ripper can also prove useful in a forensic investigation where an unknown password may hold investigators from important data.\u003c/p\u003e","title":"John the Ripper"},{"content":"PJPT Overview TCM Security\u0026rsquo;s Practical Junior Penetration Tester is a training program designed to offer hands-on penetration testing material and labs. The course covers essential tools and techniques that are exercised through labs and taught through lectures. PJPT focuses on developing the ability to identify and exploit vulnerabilities.\n","permalink":"http://karltrowbridge1.github.io/projects/pjpt/","summary":"\u003ch1 id=\"pjpt-overview\"\u003ePJPT Overview\u003c/h1\u003e\n\u003cp\u003eTCM Security\u0026rsquo;s \u003ca href=\"https://certifications.tcm-sec.com/pjpt/\"\u003ePractical Junior Penetration Tester\u003c/a\u003e is a training program designed to offer hands-on penetration testing material and labs. The course covers essential tools and techniques that are exercised through labs and taught through lectures. PJPT focuses on developing the \u003cem\u003eability\u003c/em\u003e to identify and exploit vulnerabilities.\u003c/p\u003e","title":"PJPT"},{"content":"Spotbot Overview Spotbot is a Discord bot that scrapes links sent to a Discord channel and automatically adds songs to a Spotify playlist.\n","permalink":"http://karltrowbridge1.github.io/projects/spotbot/","summary":"\u003ch1 id=\"spotbot-overview\"\u003eSpotbot Overview\u003c/h1\u003e\n\u003cp\u003e\u003ca href=\"https://github.com/wildxmxtt/spotbot2\"\u003eSpotbot\u003c/a\u003e is a Discord bot that scrapes links sent to a Discord channel and automatically adds songs to a Spotify playlist.\u003c/p\u003e","title":"SpotBot"},{"content":"Spotbot 1 Spotbot is a Discord bot that scrapes Spotify links from a Discord channel and automatically adds the songs to a set Spotify playlist. Users must set up and host the bot on their own hardware. To authenticate, user login with a Spotify account and set all parameters in a JSON file. In its original state the bot utilizes files for storage of links sent in the chat.\nWhen Matt, my college friend started applying to jobs he wanted to again start development to have a project to talk about in interviews. He decided to ask for help from some of his friends and create a new GitHub for use to work on.\nUpgrades The big upgrades planned:\nStore data in database: allows for new features and storage of meta-data Spotbot Installer: simplifies installation process Rate limiting: avoids issues with several songs being sent in chat Leaderboards: \u0026ldquo;gameify\u0026rdquo; the use of the app Milestones / achievements: \u0026ldquo;gameify\u0026rdquo; the use of the app We set up a Kanban and got to work. My first order of business was working on the database to store the links sent in the chat and collect more metadata for later analysis or features.\nDatabase The application already had some overhead in the form of hosting and configuration. Weighing these characteristics I chose SQLite for its lightweight nature and Python library for integration.\nThis required the baseline creation of the database for the storage of links, messages, timestamps, and users responsible for said links. Any portion of the application that previously interfaced with the file now had to be updated to connect to the database and query for results. Notable aspects of previously established functions that had to be re-written were the following:\nRandom song function Duplicate song check URI staging file function It goes without saying that after transitioning code to interface with this database, proceeding functions will also need to interface with the database.\nA small side note on databases, the playlist duration milestones function also uses the database. The playlist duration milestones are measured in hours from 1 all the way to 1000 and need persistence if the server for some reason has to restart. This could have been achieved through JSON but another team member is theorizing that cross-platform issues between Linux and Windows are due to JSON writing. Thus, a database table was created to keep track of achievements.\nLeaderboards The ideas for leaderboards help \u0026ldquo;gameify\u0026rdquo; the application by adding leaderboards for songs given by each user. To cover bases for all sizes of servers, several types of leaderboards were made:\nA leaderboard for all-time stats A this-month leaderboard for stats on the current month A reaction champion for highest reacted songs for the current month All-Time The first leaderboard for all time stats was made possible through the use of basic SQL. Utilizing the harvested sender_id from each filed Spotify link, the application can group all songs sent by the same user together. This is then filled into an embed to be sent to the chat that is formatted in a presentable fashion, alongside a link to the playlist itself.\nThis Month The second leaderboard that is limited to the current month makes use of the timestamp field in the database. As each link is filed in the database, the exact time stamp is recorded from the actual Discord message itself. This is made possible through discords data. when the message is recorded, the timestamp that it was sent to the Discord server using msg.created_at. I then format this to year-month-day hour:minute:second. Using the current year and month the SQL select statement can return only songs from this time period and count the number of links sent by each individual user.\nReactions this Month Finally, the last leaderboard is the reaction champion leaderboard. In Discord, users can add \u0026ldquo;reactions\u0026rdquo; to messages, where an emoji will be displayed under the respective message. Other users can click this reaction to add another reaction to it. This leaderboard utilizes the discord_message_id, a metric that our database collects that references the ID of the actual message on Discords servers. The usefulness of this metric offers the exact messages that need to be looked at. instead of parsing the entire log of messages, the bot can skip messages that do not count and begin the tally on messages that contain links sent containing Spotify songs.\nAchievements For achievements the app uses 2 metrics; number of songs in the playlist and playlist duration.\nNumber of Songs The first and most simple of the 2 is the number of songs in the playlist. Every 5 messages, the number of songs is compared to the pre-set number of milestones that grow in size from 25 songs to the 100\u0026rsquo;s of songs. If one of these numbers is met, a message is crafted and sent from the bot in celebration.\nPlaylist Duration For the more complex of the 2 we have the playlist duration achievements. Again as previously stated, the playlist duration milestones function also uses the database. This requires the use of the Spotify API.\nAll of the work regarding the initial access to Spotify\u0026rsquo;s API, token handling, and token renewal, was already written and in a working state. This whole portion needed to be rewritten to better promote abstraction to pass a spotipy object to be used by the duration achievement portion of the program. In short, many aspects of the playlist_update.py file were moved around and rewritten to break up processes into smaller easier to digest methods.\nWith these methods, the spotipy object can be retrieved and used by achievements.py to get the playlist items from Spotify\u0026rsquo;s API and begin the process of summing the total duration in hours. This is then returned. The problem that separates this issue from the initial number of songs achievement is the fact that the playlist could potentially be the length of an achievement for several checks.\nUsing the database created, if the length in hours matches a milestone the database will record an associated timestamp for each milestone respectively. When the length is checked again and the duration is still the same milestone the timestamp will be populated and no longer send out a celebration message.\nSummary Spotbot is a Discord bot that adds Spotify links from a Discord channel to a Spotify playlist. The initial project utilized text files for its main storage of songs. With the upgrades I have done so far, the bot now uses an SQLite database for storing links, messages, timestamps, and user data. Some features I have added include leaderboards and achievements that \u0026ldquo;gameify\u0026rdquo; the app and increase user interaction.\nThese leaderboards track all-time stats, monthly stats, and reactions. The achievements are based on the number of songs and playlist duration. The database offers persistence and avoids issues with JSON. Finally I have better abstracted the Spotify API interactions within the app for better maintainability.\n","permalink":"http://karltrowbridge1.github.io/blog/spotbot1/catching-up-on-spotbot/","summary":"\u003ch1 id=\"spotbot-1\"\u003eSpotbot 1\u003c/h1\u003e\n\u003cp\u003eSpotbot is a Discord bot that scrapes Spotify links from a Discord channel and automatically adds the songs to a set Spotify playlist. Users must set up and host the bot on their own hardware. To authenticate, user login with a Spotify account and set all parameters in a JSON file. In its original state the bot utilizes files for storage of links sent in the chat.\u003c/p\u003e\n\u003cp\u003eWhen Matt, my college friend started applying to jobs he wanted to again start development to have a project to talk about in interviews. He decided to ask for help from some of his friends and create a new GitHub for use to work on.\u003c/p\u003e","title":"Catching Up on SpotBot"},{"content":"This is my first post on the blog I wanted to create a place for me to talk about weekly or monthly progress made in my free time. As of right now, I am curretnly working on a project called spotbot, and studying for a certification called PJPT along side my brother, a cybersecurity professional. Instead of flooding my LinkedIn with posts about things like this, I thought I would blog about it instead and maybe post about big milestones in this blog on said LinkedIn. We\u0026rsquo;ll see what I get up to!\n","permalink":"http://karltrowbridge1.github.io/blog/blog1/firstpost/","summary":"\u003ch2 id=\"this-is-my-first-post-on-the-blog\"\u003eThis is my first post on the blog\u003c/h2\u003e\n\u003cp\u003eI wanted to create a place for me to talk about weekly or monthly progress made in my free time. As of right now, I am curretnly working on a project called \u003ca href=\"https://github.com/wildxmxtt/spotbot2\"\u003espotbot\u003c/a\u003e, and studying for a certification called \u003ca href=\"https://certifications.tcm-sec.com/pjpt/\"\u003ePJPT\u003c/a\u003e along side my brother, a cybersecurity professional. Instead of flooding my LinkedIn with posts about things like this, I thought I would blog about it instead and maybe post about big milestones in this blog on said LinkedIn. We\u0026rsquo;ll see what I get up to!\u003c/p\u003e","title":"Firstpost"}]